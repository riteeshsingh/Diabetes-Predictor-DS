# -*- coding: utf-8 -*-
"""DIABETES.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19IyvysG7WHYzUcPNiWtAD6BKSSOYci_P
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('/content/diabetes.csv')

# Display basic information about the dataset
print(data.info())
print(data.describe())

# Check for missing values
print(data.isnull().sum())

# Plot distributions of each feature
data.hist(bins=30, figsize=(20, 15))
plt.tight_layout()
plt.show()

# Plot the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.show()

# Plot pairplot for a quick look at feature relationships
sns.pairplot(data, hue='Outcome')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Separate features and target
X = data.drop('Outcome', axis=1)
y = data['Outcome']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

# Evaluate the model
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print('Accuracy:', accuracy_score(y_test, y_pred))

!pip install pandas matplotlib seaborn scikit-learn shap lime

import shap
import pandas as pd

# Assuming rf_model is defined and trained
# Initialize the SHAP explainer
explainer = shap.TreeExplainer(rf_model)

# Assuming X_test and X are defined
# Compute SHAP values
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test, feature_names=X.columns)

# Dependence plot for a specific feature (e.g., 'Glucose') for the first class
shap.dependence_plot('Glucose', shap_values[:, :, 0], X_test, feature_names=X.columns)

# Dependence plot for a specific feature (e.g., 'Glucose') for the second class
shap.dependence_plot('Glucose', shap_values[:, :, 1], X_test, feature_names=X.columns)

# Convert X_test to a pandas DataFrame
X_test_df = pd.DataFrame(X_test, columns=X.columns)

# Force plot for a specific prediction (e.g., the first test instance) for the first class
shap.initjs()
shap.force_plot(explainer.expected_value[0], shap_values[:, :, 0][0], X_test_df.iloc[0], feature_names=X.columns)

# Force plot for a specific prediction (e.g., the first test instance) for the second class
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[:, :, 1][0], X_test_df.iloc[0], feature_names=X.columns)

print(shap_values.shape)

import matplotlib.pyplot as plt
import pandas as pd
import lime
import lime.lime_tabular

# Assuming X_train, X_test, and rf_model are defined earlier

# Initialize the LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(X_train,
                                                   feature_names=X.columns,
                                                   class_names=['Non-Diabetic', 'Diabetic'],
                                                   discretize_continuous=True)

# Explain a single prediction
i = 0  # Index of the instance you want to explain
exp = explainer.explain_instance(X_test[i], rf_model.predict_proba, num_features=10)

# Visualize the explanation
exp.show_in_notebook(show_table=True)

# Save the explanation as a pyplot figure
fig = exp.as_pyplot_figure()

# Add title and annotations
plt.title("Explanation for Prediction")
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.tight_layout()

# Save or display the figure
plt.savefig("lime_explanation.png")  # Save as an image
plt.show()  # Display in Jupyter Notebook

def classify_patient():
    while True:
        # Get user input for the index
        index = int(input("Enter the index of the patient (or enter -1 to exit): "))

        # Check if the user wants to exit
        if index == -1:
            print("Exiting the program.")
            break

        # Check if the index is valid
        if index < 0 or index >= len(data):
            print("Invalid index. Please try again.")
            continue

        # Get features and outcome for the given index
        features = data.iloc[index, :-1]  # Exclude the last column (Outcome)
        outcome = data.iloc[index, -1]    # Outcome column (0 or 1)

        # Check if the patient is diabetic or not
        classification = "\033[1mDiabetic\033[0m" if outcome == 1 else "\033[1mNon-Diabetic\033[0m"

        # Print features and their values
        print("Features and Values:")
        for feature, value in features.items():
            print(f"{feature}: {value}")

        # Print classification (bold)
        print("Classification:", classification)
        print('\n\n')

# Call the function to classify patients
classify_patient()

